{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP4(Implementation des KNNs applique au spam et ham)\n",
    "   ##  GROUPE 3 CONSTITUE DE :\n",
    "   ##        - LEPAFO MOFFO Baurel               21S2782\n",
    "   ##        - MANFOUO SELATSA Ridano            16U2848\n",
    "   ##        - TEINGA KAMGUEN Emmanuel Rufin     21S2822\n",
    "   ##        - MEBODO ONOMO JUSTIN JORDAN        19M2217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilisation des KNN pour detecter les SPAMs et les HAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importation des utilitaires necessaires\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chargement des donnees\n",
    "def load_data():\n",
    "    print(\"chargement des données...\")\n",
    "    ## on retourne la liste des fichier via os.listdir\n",
    "    ham_files_location = os.listdir(\"dataset/ham\")\n",
    "    spam_files_location = os.listdir(\"dataset/spam\")\n",
    "    data = []\n",
    "    # chargement de email de classe Ham\n",
    "    for file_path in ham_files_location:\n",
    "        f = open(\"dataset/ham/\" + file_path, \"r\") ## ici on ouvre un fichier de ham et envoi son contenu dans text\n",
    "        text = str(f.read()) ## ici on converti le contenu du fichier en string\n",
    "        data.append([text, \"ham\"])## ici on envoi la valeur du fichier de forme str dans data\n",
    "    \n",
    "    # chargement des emails de classes spam\n",
    "    for file_path in spam_files_location: ## ici on fait de meme que les ham mais pour les spam\n",
    "        f = open(\"dataset/spam/\" + file_path, \"r\") \n",
    "        text = str(f.read())\n",
    "        data.append([text, \"spam\"])\n",
    "    \n",
    "    data = np.array(data) ## on tranforme les donnees en un np.array\n",
    "    \n",
    "    print(\"Tache 1: chargement des donnees\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maintenant nous devons praitraiter nos donnees\n",
    "\n",
    "def preprocess_data(data):\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    punc = string.punctuation           ##liste des punctuations\n",
    "    sw = stopwords.words('english')     ##liste des Stopwords\n",
    "    for record in data:\n",
    "        ## suppression de ponctuation et symboles standard\n",
    "        for item in punc:\n",
    "            record[0] = record[0].replace(item, \"\")\n",
    "        \n",
    "        # on met tous les mots en minuscules et on supprime les stopwords \n",
    "        splittedWords = record[0].split() ## split ici est pour obtenir la liste de tous les  emails\n",
    "        newText = \"\"\n",
    "        for word in splittedWords:\n",
    "            if word not in sw: ## on evite de prendre dans notre liste de mot des symbol non desire\n",
    "                word = word.lower()\n",
    "                newText = newText + \" \" + word  \n",
    "        record[0] = newText\n",
    "        \n",
    "    print(\"Tache 2: pretraitement des donnees\")        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoupage des donnees en ensembles d'entrainnement et de test\n",
    "def split_data(data):\n",
    "    print(\"Decoupage des donnees...\")\n",
    "    \n",
    "    features = data[:, 0]   # Tableau des emails\n",
    "    labels = data[:, 1]     # Tableau des labels\n",
    "    print(labels)\n",
    "    training_data, test_data, training_labels, test_labels =\\\n",
    "        train_test_split(features, labels, test_size = 0.27, random_state = 42)\n",
    "    \n",
    "    print(\"tache 3: donnees decoupees\")\n",
    "    return training_data, test_data, training_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creation de la fonction comptant le nombre d'occurence d'un mot via l'outil de str split\n",
    "def get_count(text):\n",
    "    wordCounts = dict()\n",
    "    for word in text.split():\n",
    "        if word in wordCounts:\n",
    "            wordCounts[word] += 1\n",
    "        else:\n",
    "            wordCounts[word] = 1\n",
    "    \n",
    "    return wordCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ici on calcule la distance euclidienne entre les observations de l'ensemble de test et ceux de l'ensemble d'Entrainement\n",
    "def euclidean_difference(test_WordCounts, training_WordCounts):\n",
    "    total = 0\n",
    "    for word in test_WordCounts:\n",
    "        if word in test_WordCounts and word in training_WordCounts:\n",
    "            total += (test_WordCounts[word] - training_WordCounts[word])**2 ## mise au carre de la difference de distance de mot\n",
    "            del training_WordCounts[word] ## suppression du mot commun tuiliser dans les 2 ensembles\n",
    "        else:\n",
    "            total += test_WordCounts[word]**2 ## pas besoin de calculer la difference vu que le mot est dans un seul endroit\n",
    "            \n",
    "    for word in training_WordCounts:\n",
    "        total += training_WordCounts[word]**2\n",
    "        \n",
    "    return total**0.5 ## ici on y fait la racine carre de la valeur totale pour avoir la difference finalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retourner la classe du mail correspondant(soit un spam soit un ham selon le nombre)\n",
    "def get_class(selected_Kvalues):\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    \n",
    "    for value in selected_Kvalues:\n",
    "        if value[0] == \"spam\":\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            ham_count += 1\n",
    "    if spam_count > ham_count:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classifieur knn\n",
    "\n",
    "def knn_classifier(training_data, training_labels, test_data, K, tsize):\n",
    "    print(\"Lancement du classifier KNN...\")\n",
    "    \n",
    "    result = []\n",
    "    counter = 1\n",
    "    \n",
    "    # compteur de mots pour entrainement des emails\n",
    "    \n",
    "    training_WordCounts = [] \n",
    "    for training_text in training_data:\n",
    "            training_WordCounts.append(get_count(training_text))\n",
    "    for test_text in test_data:\n",
    "        \n",
    "        similarity = [] #liste des similarite via la distance euclidienn\n",
    "        test_WordCounts = get_count(test_text)  #compteur de mot pour des emails\n",
    "        \n",
    "        # retourner la distance euclidienne entre ensemble \n",
    "       \n",
    "        for index in range(len(training_data)):\n",
    "            euclidean_diff =\\\n",
    "                euclidean_difference(test_WordCounts, training_WordCounts[index])\n",
    "            similarity.append([training_labels[index], euclidean_diff])\n",
    "        \n",
    "        # on tri ici la liste a l'Aide des differences euclidienne\n",
    "        \n",
    "        similarity = sorted(similarity, key = lambda i:i[1])\n",
    "        \n",
    "        #Selection de K plus proche voisins\n",
    "        \n",
    "        selected_Kvalues = [] \n",
    "        for i in range(K):\n",
    "            selected_Kvalues.append(similarity[i])\n",
    "            \n",
    "            #Class predite pour les emails\n",
    "        \n",
    "        result.append(get_class(selected_Kvalues))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chargement des données...\n",
      "Tache 1: chargement des donnees\n",
      "Preprocessing data...\n",
      "Tache 2: pretraitement des donnees\n",
      "Decoupage des donnees...\n",
      "['ham' 'ham' 'ham' ... 'spam' 'spam' 'spam']\n",
      "tache 3: donnees decoupees\n",
      "Lancement du classifier KNN...\n",
      "training data size\t: 4275\n",
      "test data size\t: 1582\n",
      "K value\t: 11\n",
      "Samples tested\t: 1582\n",
      "% accuracy\t:76.73830594184577\n",
      "Number correct\t: 1214\n",
      "Number wrong\\: 368\n"
     ]
    }
   ],
   "source": [
    "## mise en oeuvre de notre classifieur\n",
    "def main(K):\n",
    "    data = load_data()\n",
    "    data = preprocess_data(data)\n",
    "    training_data, test_data, training_labels, test_labels = split_data(data)\n",
    "    tsize = len(test_data)\n",
    "    result = knn_classifier(training_data, training_labels, test_data[:tsize], K, tsize) \n",
    "    accuracy = accuracy_score(test_labels[:tsize], result)\n",
    "    print(\"training data size\\t: \" + str(len(training_data)))\n",
    "    print(\"test data size\\t: \" + str(len(test_data)))\n",
    "    print(\"K value\\t: \" + str(K))\n",
    "    print(\"Samples tested\\t: \" + str(tsize))\n",
    "    print(\"% accuracy\\t:\" + str(accuracy * 100))\n",
    "    print(\"Number correct\\t: \" + str(int(accuracy * tsize)))\n",
    "    print(\"Number wrong\\: \" + str(int((1 - accuracy) * tsize)))\n",
    "\n",
    "main(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
